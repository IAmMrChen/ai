{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python爬虫(crawler)与数据采集\n",
    "\n",
    "### 面授班讲义  by 杨庆跃\n",
    "请安装最新版本Jupyter notebook进行编程实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关于网络爬虫\n",
    "网络爬虫（又被称为网页蜘蛛，网络机器人，网页追逐者），是一种按照一定的规则，自动地抓取互联网信息的程序或者脚本。  \n",
    "维基百科描述如下：[documentation](https://en.wikipedia.org/wiki/Web_crawler)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录\n",
    "- 爬虫原理\n",
    "- Requests库介绍\n",
    "- 防止封IP的方法\n",
    "- 常用技巧\n",
    "- 未来需要处理的难点：动态验证码\n",
    "- beautiful soup4介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _*_ coding:utf-8 _*_\n",
    "import urllib.request as req\n",
    "\n",
    "#向指定的url地址发送请求，并返回服务器响应的类文件对象\n",
    "response = req.urlopen('http://www.baidu.com/')\n",
    "#服务器返回的类文件对象支持python文件对象的操作方法\n",
    "#read()方法就是读取文件里的全部内容，返回字符串\n",
    "html = response.read()\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requests库\n",
    "#### \"HTTP for Humans\"\n",
    "Requests是基于urllib的第三方Python库，Requests比urllib更加方便，可以节约我们大量的工作。  \n",
    "Requests具有强大的网络请求功能，可以实现跟浏览器一样发送各种HTTP请求来获取网站的数据，经常用来编写爬虫和测试服务器响应数据。  \n",
    "\n",
    "[Requests官网](http://docs.python-requests.org/en/master/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【课堂讲解】HTTP协议、请求中的GET和POST方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最简单示例\n",
    "import requests\n",
    " \n",
    "response  = requests.get(\"http://www.baidu.com\")\n",
    "response.encoding = \"utf-8\" #response.text返回的是Unicode格式，通常需要转换为utf-8格式，否则就是乱码\n",
    "print(response.text) #打印网页内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### response.status_code:返回的状态码，显示请求是否成功  \n",
    "2开头 （请求成功）表示成功处理了请求的状态代码。\n",
    "\n",
    "200   （成功）  服务器已成功处理了请求。 通常，这表示服务器提供了请求的网页。 \n",
    "201   （已创建）  请求成功并且服务器创建了新的资源。 \n",
    "202   （已接受）  服务器已接受请求，但尚未处理。 \n",
    "203   （非授权信息）  服务器已成功处理了请求，但返回的信息可能来自另一来源。 \n",
    "204   （无内容）  服务器成功处理了请求，但没有返回任何内容。 \n",
    "205   （重置内容） 服务器成功处理了请求，但没有返回任何内容。\n",
    "206   （部分内容）  服务器成功处理了部分 GET 请求。\n",
    "\n",
    "3开头 （请求被重定向）表示要完成请求，需要进一步操作。 通常，这些状态代码用来重定向。\n",
    "\n",
    "300   （多种选择）  针对请求，服务器可执行多种操作。 服务器可根据请求者 (user agent) 选择一项操作，或提供操作列表供请求者选择。 \n",
    "301   （永久移动）  请求的网页已永久移动到新位置。 服务器返回此响应（对 GET 或 HEAD 请求的响应）时，会自动将请求者转到新位置。\n",
    "302   （临时移动）  服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。\n",
    "303   （查看其他位置） 请求者应当对不同的位置使用单独的 GET 请求来检索响应时，服务器返回此代码。\n",
    "304   （未修改） 自从上次请求后，请求的网页未修改过。 服务器返回此响应时，不会返回网页内容。 \n",
    "305   （使用代理） 请求者只能使用代理访问请求的网页。 如果服务器返回此响应，还表示请求者应使用代理。 \n",
    "307   （临时重定向）  服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。\n",
    "\n",
    "4开头 （请求错误）这些状态代码表示请求可能出错，妨碍了服务器的处理。\n",
    "\n",
    "400   （错误请求） 服务器不理解请求的语法。 \n",
    "401   （未授权） 请求要求身份验证。 对于需要登录的网页，服务器可能返回此响应。 \n",
    "403   （禁止） 服务器拒绝请求。\n",
    "404   （未找到） 服务器找不到请求的网页。\n",
    "405   （方法禁用） 禁用请求中指定的方法。 \n",
    "406   （不接受） 无法使用请求的内容特性响应请求的网页。 \n",
    "407   （需要代理授权） 此状态代码与 401（未授权）类似，但指定请求者应当授权使用代理。\n",
    "408   （请求超时）  服务器等候请求时发生超时。 \n",
    "409   （冲突）  服务器在完成请求时发生冲突。 服务器必须在响应中包含有关冲突的信息。 \n",
    "410   （已删除）  如果请求的资源已永久删除，服务器就会返回此响应。 \n",
    "411   （需要有效长度） 服务器不接受不含有效内容长度标头字段的请求。 \n",
    "412   （未满足前提条件） 服务器未满足请求者在请求中设置的其中一个前提条件。 \n",
    "413   （请求实体过大） 服务器无法处理请求，因为请求实体过大，超出服务器的处理能力。 \n",
    "414   （请求的 URI 过长） 请求的 URI（通常为网址）过长，服务器无法处理。 \n",
    "415   （不支持的媒体类型） 请求的格式不受请求页面的支持。 \n",
    "416   （请求范围不符合要求） 如果页面无法提供请求的范围，则服务器会返回此状态代码。 \n",
    "417   （未满足期望值） 服务器未满足\"期望\"请求标头字段的要求。\n",
    "\n",
    "5开头（服务器错误）这些状态代码表示服务器在尝试处理请求时发生内部错误。 这些错误可能是服务器本身的错误，而不是请求出错。\n",
    "\n",
    "500   （服务器内部错误）  服务器遇到错误，无法完成请求。 \n",
    "501   （尚未实施） 服务器不具备完成请求的功能。 例如，服务器无法识别请求方法时可能会返回此代码。 \n",
    "502   （错误网关） 服务器作为网关或代理，从上游服务器收到无效响应。 \n",
    "503   （服务不可用） 服务器目前无法使用（由于超载或停机维护）。 通常，这只是暂时状态。 \n",
    "504   （网关超时）  服务器作为网关或代理，但是没有及时从上游服务器收到请求。 \n",
    "505   （HTTP 版本不受支持） 服务器不支持请求中所用的 HTTP 协议版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response  = requests.get(\"https://www.baidu.com\")\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### response.content 返回网页的原始内容\n",
    "通常需要转成utf8编码查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content[:1000] #返回前2000个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content[:1000].decode(\"utf-8\")) #rdecode转码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用带参数的GET请求爬取页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'category': '1'}\n",
    "response = requests.get(\"http://top.baidu.com\",params=params)\n",
    "print(response.content.decode(encoding ='GBK',errors = 'ignore')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用带参数的POST请求爬取页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "data = {\n",
    "    \"name\":\"zhaofan\",\n",
    "    \"age\":23\n",
    "}\n",
    "response = requests.post(\"http://httpbin.org/post\",data=data)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### response返回的常用数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.baidu.com/\n",
      "200\n",
      "{'Cache-Control': 'private, no-cache, no-store, proxy-revalidate, no-transform', 'Connection': 'Keep-Alive', 'Content-Encoding': 'gzip', 'Content-Type': 'text/html', 'Date': 'Tue, 11 Sep 2018 07:42:42 GMT', 'Last-Modified': 'Mon, 23 Jan 2017 13:23:50 GMT', 'Pragma': 'no-cache', 'Server': 'bfe/1.0.8.18', 'Set-Cookie': 'BDORZ=27315; max-age=86400; domain=.baidu.com; path=/', 'Transfer-Encoding': 'chunked'}\n",
      "<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(\"https://www.baidu.com\")\n",
    "print(response.url)\n",
    "print(response.status_code)\n",
    "print(response.headers)\n",
    "print(response.cookies)\n",
    "print(response.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【防止封IP】加headers\n",
    "采集页面时建议模拟头部信息，以防被目标网站封IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.81 Safari/537.36\",\n",
    "}\n",
    "\n",
    "params = {'category': '1'}\n",
    "response = requests.get(\"http://top.baidu.com\",params=params,headers=headers)\n",
    "print(response.content.decode(encoding ='GBK',errors = 'ignore')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【防止封IP】使用代理服务器爬取网页\n",
    "爬虫爬取数据时为避免被封IP，可以使用代理。requests支持proxies属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "proxies = {\n",
    "  \"http\": \"http://218.60.8.99:3129\",\n",
    "  \"http\": \"http://144.76.62.29:3128\",\n",
    "  \"http\": \"http://47.105.92.173:80\",\n",
    "  \"http\": \"http://47.104.222.190:80\",\n",
    "  \"http\": \"http://47.105.89.116\"\n",
    "}\n",
    "r= requests.get(\"http://www.73ke.com\", proxies=proxies)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模拟登录\n",
    "一些网站需要登录后才能抓取数据，requests支持提交表单(包含用户名和密码)进行登录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意，具体登录地址、用户名和密码的字段名因网站不同，下面仅仅是一个示例\n",
    "s = requests.session()\n",
    "data = {'user':'用户名','passdw':'密码'}\n",
    "#post 换成登录的地址，\n",
    "res=s.post('http://www.xxx.com/index.php?action=login',data);\n",
    "# 换成抓取的地址\n",
    "s.get('http://www.xxx.com/doc/');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置timeout\n",
    "设置timeout属性设置超时时间，超时会提示错误，以防止无限期等待。以秒为单位。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('http://www.73ke.com', timeout=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup\n",
    "Beautiful Soup是用Python写的一个HTML/XML的解析器，它可以兼容不规范标记并生成解析树(parse tree)。 它提供简单常用导航、搜索以及修改解析树的操作。大大节约进行网页分析的时间。\n",
    "Beautiful Soup官方网址[文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)  \n",
    "下面是简单示例，使用prettify函数重新整理HTML文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <p>\n",
      "   Click\n",
      "   <a href=\"http://www.example.com\" id=\"info\">\n",
      "    here\n",
      "   </a>\n",
      "   for more information.\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html><body><p>\n",
    "Click <a id='info' href='http://www.example.com'>here</a>\n",
    "for more information.\n",
    "</p></body></html>\n",
    "\"\"\"\n",
    "#创建 Beautiful Soup 对象\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "#格式化输出 soup 对象的内容\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 页面中的Tag对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"http://www.example.com\" id=\"info\">here</a>\n",
      "<class 'bs4.element.Tag'>\n",
      "a\n",
      "here\n",
      "{'id': 'info', 'href': 'http://www.example.com'}\n",
      "http://www.example.com\n"
     ]
    }
   ],
   "source": [
    "a_tag = soup.p.a\n",
    "print(a_tag, type(a_tag), a_tag.name,a_tag.string,a_tag.attrs,a_tag.attrs['href'],sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过下面属性遍历文档\n",
    "- parent 父节点\n",
    "- parents 祖先节点\n",
    "- next_sibling 下一个兄弟\n",
    "- next_siblings 下面所有兄弟\n",
    "- previous_sibling 上一个兄弟\n",
    "- previous_siblings 前边所有兄弟\n",
    "- contents 子节点的列表\n",
    "- children 子节点\n",
    "- descendants 所有后代  \n",
    "【课堂练习】针对下面文档，选择几个函属性看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   The Dormouse's story\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\" name=\"dromouse\">\n",
      "   <b>\n",
      "    The Dormouse's story\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   Once upon a time there were three little sisters; and their names were\n",
      "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
      "    <!-- Elsie -->\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
      "    Lacie\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
      "    Tillie\n",
      "   </a>\n",
      "   ;\n",
      "and they lived at the bottom of a well.\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   ...\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\" name=\"dromouse\"><b>The Dormouse's story</b></p>\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"><!-- Elsie --></a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "\n",
    "#创建 Beautiful Soup 对象\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.com/lacie\n"
     ]
    }
   ],
   "source": [
    "print(soup.a.next_sibling.next_sibling['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搜索文档树\n",
    "#### find_all( name , attrs , recursive , text , **kwargs )\n",
    "返回一个列表类型\n",
    "- name 查找所有名字为 name 的tag,字符串对象会被自动忽略掉\n",
    "- attrs 定义一个字典参数来搜索包含特殊属性的tag\n",
    "- text 文档中的字符串内容\n",
    "- recursive 是否递归到子孙节点  \n",
    "-  **kwargs 其他参数\n",
    "还有其他类似函数，find()、find_parents、find_next_siblings等  \n",
    "【课堂练习】使用find_函数搜索节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all('a',id='link3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSS选择器\n",
    "- soup.select('a') #通过name查找\n",
    "- soup.select('.sister') #通过类名查找\n",
    "- soup.select('#link3') #通过id查找\n",
    "- soup.select('p #link1') #组合查找\n",
    "- soup.select('a[class=\"sister\"]') #属性查找  \n",
    "\n",
    "select 方法返回的结果都是列表形式，可以遍历形式输出，然后用 get_text() 方法来获取它的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【课堂练习】使用select函数搜索节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lacie\n",
      "Tillie\n"
     ]
    }
   ],
   "source": [
    "for title in soup.select('a'):\n",
    "    print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
